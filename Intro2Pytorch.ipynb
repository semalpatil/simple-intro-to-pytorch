{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4827160",
   "metadata": {
    "id": "f4827160"
   },
   "source": [
    "# A Practical Introduction to Learning ML using Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbc3c10",
   "metadata": {
    "id": "ffbc3c10"
   },
   "source": [
    "This is a simple introduction to Machine Learning using Pytorch. Before using Pytorch, we will first take a quick look at linear regression using Scikit-Learn a machine learning toolkit, based on a traditional non-neural net approach. This will give you a good idea about the problem we are trying to solve, and help you understand the Pytorch code easily."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381542cd",
   "metadata": {
    "id": "381542cd"
   },
   "source": [
    "## Linear Regression: The Scikit Way ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a2279f",
   "metadata": {
    "id": "34a2279f"
   },
   "source": [
    "\n",
    "As we know the equation of a line is: <br/> $y = mX + c$ <br/> where **m** is the slope of the line and **c** is the constant or the intercept. In machine learning terminology the _m_ is usually represented as a weight or **w** and _C_ is represented as a bias **b**. <br/> _*Linear Regression*_ is a statistical process for determining a line that best fits the given data, so that we can predict the output for any other unseen input values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c512310b",
   "metadata": {
    "id": "c512310b"
   },
   "source": [
    "Most of the tutorials out there, start with some dataset and try to fit the data. This is a good approach, as in real life you will generally not find any dataset that perfectly follows an equation. But most datasets in real life must be pre-processed before it can be used. But all that pre-processing, adds some overhead to the program and we may get distracted from the important part of the code. Hence here we will just syntesize our own data and compare the, \"w\" and \"b\" which we alrady know, with scikit learn model parameters after its done training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7188fef7",
   "metadata": {
    "id": "7188fef7"
   },
   "source": [
    "Let us assume that the value of m is 1.29 and c is 24. In that case we can define our function to generate the desired value of y based on a value of x as follows:\n",
    "<br/>\n",
    "$y = 1.29x + 24 $\n",
    "\n",
    "Let us use numpy to create random values. We will then define a function to synthesize the output based on the input.\n",
    "To create our output _y_ values ( a.k.a _labels_ ) we can use the list comprehension syntax to generate lots of y values corresponding to the input x values.\n",
    "<br/><br/>If this list comprehension syntax is a bit too much for you  or you simply need a crash course in Python or even a quick refresher before you dive into the code then I would highly recommend this amazing online resource: <a href=\"https://www.pythonlikeyoumeanit.com/\"> https://www.pythonlikeyoumeanit.com/ </a>\n",
    "\n",
    "Consider the following code [In] #1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85c6d26a",
   "metadata": {
    "id": "85c6d26a"
   },
   "outputs": [],
   "source": [
    "# synthesize the training data from our own line formula\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "x_train = np.float32(np.random.rand(50000,1)*10)\n",
    "\n",
    "def compute_output(input):\n",
    "    return (1.29*input[0] + 24)\n",
    "\n",
    "y_train = np.array([compute_output(row) for row in x_train]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568cc129",
   "metadata": {
    "id": "568cc129"
   },
   "source": [
    "Now that we have the Training Data, let us see how we can use sklearn to solve this simple Linear Regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a34c0f05",
   "metadata": {
    "id": "a34c0f05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight: [[1.29]]\n",
      "Bias (Constant) : [23.999998]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import  LinearRegression\n",
    "\n",
    "lm = LinearRegression()\n",
    "\n",
    "lm.fit(x_train, y_train)\n",
    "\n",
    "# finally check the weight and constant\n",
    "\n",
    "lm.score(x_train, y_train)\n",
    "print(f\"Weight: {lm.coef_}\")\n",
    "print(f\"Bias (Constant) : {lm.intercept_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c180c90c",
   "metadata": {
    "id": "c180c90c"
   },
   "source": [
    "As you can see here that sklearn finds the coefficient a.k.a weight and the bias correctly, so we need *not* again verify the results with some test inputs.\n",
    "In real life problems though, we do not always have just one feature or input variable, we will typically have many varibles. But the linear regression problem remains almost the same and the solution can be extended to multiple variables very easily. Here we are using 4 input variables, numpy helps us to create 50000 random values for each of these 4 variables. Since we have 4 input variables we will obviously have 4 weights. Let us say that the weights are, 1.29, 3.1, 2.22 and 1.9. The b is the same as last time i.e. 13. So our compute_output function can now be mathematically represented as : <br>\n",
    "$$ y = 1.29 x_{1} + 3.1 x_{2} + 2.22x_{3} + 1.9x_{4}+13 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a9c5a37",
   "metadata": {
    "id": "6a9c5a37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.2900004 3.1000004 2.22      1.9000006]]\n",
      "[12.999985]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import  LinearRegression\n",
    "x_train = np.float32(np.random.rand(50000,4)*10)\n",
    "\n",
    "def compute_output(input):\n",
    "    return (1.29*input[0] + 3.1 * input[1] + 2.22 * input[2] + 1.9*input[3]  +  13)\n",
    "\n",
    "y_train = np.array([compute_output(row) for row in x_train]).reshape(-1,1)\n",
    "\n",
    "lm = LinearRegression()\n",
    "\n",
    "lm.fit(x_train, y_train)\n",
    "\n",
    "# finally check the weight and constant (bias)\n",
    "\n",
    "lm.score(x_train, y_train)\n",
    "print(lm.coef_)\n",
    "print(lm.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f94224e",
   "metadata": {
    "id": "8f94224e"
   },
   "source": [
    "Now if you are anything like me your first obvious reaction will be, \"Wait, when we are solving any real life problem, what is the guarantee that the relation between input and output is always linear, what if y depends on the $5^{th}$ power of some input variable\". It turns out that even that problem is very similar and simple.\n",
    "Let us consider a single variable problem that involves a higher power of the input variable.\n",
    "  $y = mx^{3} + c$  </br>\n",
    " OR in Python syntax  **y = 5.2 * x ** 3 + 31**\n",
    "\n",
    "   The trick here is to synthesize multiple variables from a single variable, using powers. For example even if we know that there is only one input variable x, we can say that this equation really has 4 input varibles $x$, $x^{2}$, $x^{3}$, $x^{4}$ . If you are really paying attention, you may ask \"why $x^{4}$ ?\". I am just trying to emphasize the point that in real life scenarios you may not know if the relation between input involves a power of 3 or power of 4. So let us take a guess and go with 4. Any powers that are not involved will get an almost ZERO coefficient and hence will not be used. So if you really think, now its again the same problem as above; it is a simple linear regression with 4 input variables. We will first see the non sklearn way of achieving it and the use the proper sklearn way of doing the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d1ce89f",
   "metadata": {
    "id": "4d1ce89f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Shape of Input Features (50000, 1)\n",
      "We fabricated additional features by using powers of the only input variable. New Shape of Input Features (50000, 4)\n",
      "The 4 Weights: [[ 2.8312812e-05 -2.3245811e-06  5.2000022e+00 -5.9604645e-08]]\n",
      "The Bias: [31.015137]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import  LinearRegression\n",
    "\n",
    "x_train = np.float32(np.random.rand(50000,1)*10)\n",
    "\n",
    "# Syntesize output from the random inputs\n",
    "def compute_output(input):\n",
    "    return (5.2 * ( input[0] ** 3)  + 31 )\n",
    "\n",
    "y_train = np.array([compute_output(row) for row in x_train]).reshape(-1,1)\n",
    "\n",
    "print(f\"Original Shape of Input Features {x_train.shape}\")\n",
    "\n",
    "# Magic Trick: assume that the powers may be up to 4 and fabricate additional features using powers\n",
    "\n",
    "poly_x = np.array([(x, x**2, x**3, x**4) for x in x_train]).reshape(-1,4)\n",
    "\n",
    "print(f\"We fabricated additional features by using powers of the only input variable. New Shape of Input Features {poly_x.shape}\")\n",
    "\n",
    "lm = LinearRegression()\n",
    "\n",
    "lm.fit(poly_x, y_train)\n",
    "\n",
    "# finally check the weight and constant  ( bias)\n",
    "\n",
    "lm.score(poly_x, y_train)\n",
    "print(f\"The 4 Weights: {lm.coef_}\")\n",
    "print(f'The Bias: {lm.intercept_}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e962a412",
   "metadata": {
    "id": "e962a412"
   },
   "source": [
    "Notice that the coefficients or weights of the other powers are really small numbers something like -4.7960515e-07 ( something with a $ 10^{-7}$ which is a really small number). Which simply means that this power is inconsequential or simply not present in the actual equation that describes relation between input and output. As you can see it found out the coefficient of $ x^{3}$ correctly.\n",
    "Now one last bit from the Sklearn world before we jump into the deep downword slope of gradient descent; sklearn preprocessing offers a nicer way of expanding the features from a single variable to multiple variables using the powers. Here is the proper sklearn way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19bf30f6",
   "metadata": {
    "id": "19bf30f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Shape of Input Features (50000, 1)\n",
      "New Shape of Input Features (50000, 5)\n",
      "The Weights are: [[ 0.0000000e+00  1.5395880e-04  4.5483932e-05  5.1999965e+00\n",
      "  -5.9604645e-08]]\n",
      "The Bias is: [31.016724]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import  LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "x_train = np.float32(np.random.rand(50000,1)*10)\n",
    "\n",
    "# Compute output from the randomly generated input values\n",
    "def compute_output(input):\n",
    "    return (5.2 * ( input[0] ** 3)  + 31 )\n",
    "\n",
    "y_train = np.array([compute_output(row) for row in x_train]).reshape(-1,1)\n",
    "\n",
    "print(f\"Original Shape of Input Features {x_train.shape}\")\n",
    "\n",
    "# Magic Trick: assume that the equation may involve powers maybe up to 4 and fabricate additional features using powers\n",
    "\n",
    "poly = PolynomialFeatures(4) # we assume that it may contain any powers upto 4, hence that 4 in the input\n",
    "\n",
    "# The raw way: poly_x = np.array([(x, x**2, x**3, x**4) for x in x_train]).reshape(-1,4)\n",
    "# the proper sklearn preprocessing  way\n",
    "poly_x = poly.fit_transform(x_train)\n",
    "\n",
    "print(f\"New Shape of Input Features {poly_x.shape}\")\n",
    "\n",
    "\n",
    "lm = LinearRegression()\n",
    "\n",
    "lm.fit(poly_x, y_train)\n",
    "\n",
    "# finally check the weight and bias\n",
    "\n",
    "lm.score(poly_x, y_train)\n",
    "print(f'The Weights are: {lm.coef_}')\n",
    "print(f'The Bias is: {lm.intercept_}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6396a212",
   "metadata": {
    "id": "6396a212"
   },
   "source": [
    "##  Starting from the Scratch: Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6349b5",
   "metadata": {
    "id": "2c6349b5"
   },
   "source": [
    "There are a lot of resources out there which cover the theory aspects about gradient descent, the learning rate, the loss function and all that. I specifically wanted to keep the **focus on the actual code**. Sometimes the higher level abtractions provided by a ML library make it overwhelming. In the following section we will implement everything from scratch without using any ML library, ofcourse we will still use numpy to syntesize the data and manipulate that data. What I mean is there is No sklearn, No pytorch, No TensorFlow, No MXNet or JAX here. Once you understand some of these basics and how they have evolved into those higher level constructs to take care of a specific part of the code, then the code will look much more easier to understand and modify. Let us consider exactly same problem as Code #1.\n",
    "<br/><br/> In case you need a super quick refresher then here is the gist. The following code uses the gradient descent technique to reduce the loss by adjusting the weight and the bias. We adjust the weight and bias based on the learning rate. Learning Rate is a hyperparameter that dictates how fast or slow we learn from the loss from the last iteration. It is called a _hyperparameter_ because it is not a real parameter in our actual equiation like the weights and the bias are, but still is very important. So basically here is how it goes; we compute the predicted values using our weight  and bias (a.k.a. using our model), then we calculate the difference between actual output and the predicted output value, calculate the loss using mean square error technique, compute the partial derivatives of the MSE loss with respect to the weights as well as the bias, ( this helps us in tuning the weight and bias to reduce the loss) then adjust the weights and bias using the learning rate and the partial derivative computed in the previous step. We run the above process through multiple iterations to reduce the loss.\n",
    "    <br><br>If you have not already read something like the above text a hundred times already then you may feel that; this is just a bunch of theoretical BS with some buzzwords thrown around. But I wanted to get to the code as soon as possible, and needed a brief expalantory text before we jump right in to the the actual code. So just go through the code and things will become a bit clearer. Some things may **not** be immediately clear ( for example why we chose the learning rate to be 0.01 ), but just go with it for now and we may touch upon those aspects later.\n",
    "\n",
    "Here we start with some random values for weight and bias.\n",
    "Since the difference between the predicted value and the actual value can either be negative or positive, we will take the square of the difference between the predicted value and the actual value, which will always yield a +ve number. Since we have lots of predicted values and actual values, we will take the mean of the square of the difference between predicted and actual values. This is how we define our loss function.\n",
    "\n",
    "We already know that : <br/><font size=\"4\" color=\"blue\">\n",
    "    $ y_{predicted} = ( x_{train} w ) + b $ </font>\n",
    "    \n",
    " and if we ignore the mean for the timebeing then the loss function is:<br/>\n",
    "    <font size=\"4\" color=\"blue\">\n",
    "   $ loss =   (y_{predicted} - y_{train}) ^{2} $\n",
    "   </font>\n",
    "    \n",
    "So if we replace the value of $y_{predicted}$ in the loss function and use the expansion formulae for the square; <br/><br/>\n",
    "<font size=\"3\" color=\"blue\">\n",
    "$(a-b) ^{2} = a^{2} - 2ab + b^{2}  $\n",
    "<br/>\n",
    "$(a+b) ^{2} = a^{2} + 2ab + b^{2}$,\n",
    "</font>\n",
    "<br/><br/>We get the folllwing :\n",
    "<br/><b>Step 1:</b><br/><font size=\"4\" color=\"blue\">\n",
    "$y_{predicted}^{2} - 2y_{predicted}y_{train} + y_{train}^{2} $ <br/>\n",
    "</font>\n",
    "<br/><b>Step 2:</b><br/>\n",
    "<font size=\"4\" color=\"blue\">\n",
    "$ (x_{train}w + b )^{2} - 2((x_{train}w + b )y_{train}) + y_{train}^{2} $ <br/>\n",
    "</font>\n",
    "<br/><b>Step 3:</b><br/>\n",
    "<font size=\"4\" color=\"blue\">\n",
    "$ loss = (x_{train}w)^{2}+2x_{train}wb + b^{2} - 2(x_{train}w + b )y_{train}) + y_{train}^{2} $ <br/><br/>\n",
    "</font>\n",
    "\n",
    "So that is our actual loss function, now as you might have read the goal is to reduce the loss by tweaking the w and b. This is where the gradient descent comes into picture. Usually when we talk about derivatives or differentiation, we are finding the derivative of a function with respect to the variable x. That is not the case here and hence may be a bit tricky to grasp at first. Our variables are the weight \"w\" and the bias \"b\". So we will be differentiating the above loss function first with respect to w and then with respect to b.\n",
    "<br/> So basically we are trying to find out these two quantities $  \\frac {dloss} {dw}  $\n",
    "and $  \\frac {dloss} {db} $\n",
    "<br/> Now take a pen and paper and try to actually find out the differentiation yourself, dont just take my word for it. So the derivative of loss with respect to w turns out to be the following: <br/><br/>\n",
    "<font size=\"4\" color=\"blue\">\n",
    "    $ \\frac{dloss}{dw} = 2 (x_{train})^{2}w + 2x_{train}b - 2x_{train}y_{train} $\n",
    "</font>\n",
    "<br/><br/> If you take out the common $ 2 x_{train}$ then it becomes: <br/><br/>\n",
    "<font size=\"4\" color=\"blue\">\n",
    "$ \\frac{dloss}{dw} = 2 (x_{train}) ( x_{train}w + b - y_{train} ) $ <br/>\n",
    "</font>\n",
    "</br> if you replace the $x_{train}w + b $ with the $ y_{predicted} $ and you get this: <br/><br/>\n",
    "<font size=\"4\" color=\"blue\">\n",
    "$ \\frac{dloss}{dw} = 2 x_{train} ( y_{predicted} - y_{train} ) $ <br/><br/></font>\n",
    "In Python code it will look something like this: <br/><br/>\n",
    " w_grad = (2 * (y_pred - y_train) * x_train ).mean()\n",
    "  <br/> <br/>Similarly if you try to solve the other differentiation you will end up with this: <br/>\n",
    "<font size=\"4\" color=\"blue\">\n",
    "$  \\frac {dloss} {db}  = 2 (y_{predicted} - y_{train})$\n",
    "</font>\n",
    "<br/><br/> In Python code it will be as follows: <br/><br/>\n",
    "   b_grad = (2 * (y_pred - y_train) ).mean()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd96497",
   "metadata": {
    "id": "5fd96497"
   },
   "source": [
    "### No Pytorch Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2066bceb",
   "metadata": {
    "id": "2066bceb"
   },
   "source": [
    "Whatever theory we discussed above is implementd below using Python and numpy. We are not using pytorch yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18171b8b",
   "metadata": {
    "id": "18171b8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, Loss=835.1124797546147\n",
      "Iter: 400, Loss=2.8193445873383\n",
      "Iter: 800, Loss=0.06034491020664517\n",
      "Iter: 1200, Loss=0.0012916151520472364\n",
      "Iter: 1600, Loss=2.7645574337339908e-05\n",
      "Iter: 2000, Loss=5.917225260403407e-07\n",
      "\n",
      "\n",
      "Weight: [[1.29023164]] \n",
      "\n",
      "Bias: [[23.99845356]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x_train = np.random.rand(5000,1)*10\n",
    "\n",
    "#Synthesize training data; we will verify the weights and bias later with the trained model\n",
    "\n",
    "def synthesize_output(input):\n",
    "    return (1.29*input[0] + 24)\n",
    "\n",
    "y_train = np.array([synthesize_output(row) for row in x_train]).reshape(-1,1)\n",
    "\n",
    "# Initialize Weights and Bias to random starting values\n",
    "\n",
    "w = np.random.rand(1,1)\n",
    "b = np.random.rand(1,1)\n",
    "\n",
    "# how much do we want to tweak the weight and bias a.k.a learning rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "for iter in range(2001):\n",
    "\n",
    "    # forward pass : predict values: assume the random weight and bias are correct and then calculate the output\n",
    "    # this clip(min=0) is also something very important, and is called an Activation Function\n",
    "    # it is used to avoid any negative values\n",
    "    # for now dont worry about it, we need it here, just make a mental note of this\n",
    "\n",
    "    y_pred = (x_train * w).clip(min=0) + b\n",
    "\n",
    "    # find loss. We know the actual output ( t_train ), so use the mean square of the difference , this is our loss function\n",
    "    loss = ((y_pred - y_train)**2).mean()\n",
    "\n",
    "    # Backword pass for computing gradients. Now as you may have read gradient is a partial derivitive;\n",
    "    # once with respect to the weight ( dW) and once with respect to the bias (dB).\n",
    "\n",
    "    w_grad = (2 * (y_pred - y_train) * x_train ).mean()\n",
    "    b_grad = (2 * (y_pred - y_train) ).mean()\n",
    "\n",
    "    # Just printing the loss to see how it is changing over the iterations, afterall we want to minimize it\n",
    "    if (iter % 400) == 0:\n",
    "        print(f\"Iter: {iter}, Loss={loss}\")\n",
    "\n",
    "    # Manually updating weights\n",
    "    w -= learning_rate * w_grad\n",
    "    b -= learning_rate * b_grad\n",
    "    \n",
    "# finally check the weight and bias\n",
    "print(f\"\\n\\nWeight: {w} \\n\\nBias: {b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069eb361",
   "metadata": {
    "id": "069eb361"
   },
   "source": [
    "As you can see we found out the weight and bias correctly. Pytorch or any of the ML libraries that are out there provide many constructs to do some of the things that we did manually here, thereby making the code much easier and cleaner. For example Pytorch offers something called as tensors that takes care of computing partial derivatives automatically. This is called automatic differentiation, it knows how to compute the gradient based on whatever operations we perform on those tensors. Before we jump into that aspect, let us just start by first using the tensors instead of numpy array. We will use the same example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4fe53b",
   "metadata": {
    "id": "4c4fe53b"
   },
   "source": [
    "### Pytorch Code 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "540cb5b4",
   "metadata": {
    "id": "540cb5b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "X_train: False \n",
      "\n",
      "Y_train: False\n",
      "Iter: 0, Loss=896.9687966400587\n",
      "Iter: 400, Loss=2.5573609038850376\n",
      "Iter: 800, Loss=0.05028300700492502\n",
      "Iter: 1200, Loss=0.0009887275694596257\n",
      "Iter: 1600, Loss=1.9441233538062694e-05\n",
      "Iter: 2000, Loss=3.887154188056094e-07\n",
      "\n",
      "\n",
      "Weights: tensor([[1.2902]], requires_grad=True) \n",
      "\n",
      "Bias: tensor([[23.9988]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "x_train = np.float32(np.random.rand(50000,1)*10)\n",
    "\n",
    "#Synthesize training data; we will verify the weights and bias later with the trained model\n",
    "def synthesize_output(input):\n",
    "    return (1.29*input[0] + 24)\n",
    "\n",
    "y_train = np.array([synthesize_output(row) for row in x_train]).reshape(-1,1)\n",
    "\n",
    "# instead of the numpy array use Pytorch tensors. Why ? Because I said so :)\n",
    "\n",
    "X_train = torch.from_numpy(x_train)\n",
    "Y_train = torch.from_numpy(y_train)\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Initialize Weights and Bias to random starting values the pytorch way:\n",
    "# but essentially we are still doing somethin very similar to np.random.rand(1,1)\n",
    "\n",
    "w = torch.rand(1, 1, requires_grad=True)\n",
    "b = torch.rand(1, 1, requires_grad=True)\n",
    "\n",
    "for iter in range(2001):\n",
    "\n",
    "    # forward pass : predict values the Pytorch way but essentially we are still doing\n",
    "    # something very similar to : y_pred = (x_train * w).clip(min=0) + b\n",
    "    # Also notice that clip(min=0) has become clamp(min=0) now\n",
    "\n",
    "    y_pred = X_train.mm(w).clamp(min=0).add(b)\n",
    "\n",
    "    # find loss the Pytorch way but essentially we are still doing\n",
    "    # something very similar to :   loss = ((y_pred - y_train)**2).mean()\n",
    "\n",
    "    loss = (y_pred - Y_train).pow(2).mean()\n",
    "\n",
    "    # Backword pass for computing gradients\n",
    "    # this we are still using the Non Pytorch way,\n",
    "\n",
    "    w_grad = (2 * (y_pred - Y_train) * X_train ).mean()\n",
    "    b_grad = (2 * (y_pred - Y_train) ).mean()\n",
    "\n",
    "    #Just printing the loss to see how it is changing over the iterations\n",
    "    if (iter % 400) == 0:\n",
    "        print(f\"Iter: {iter}, Loss={loss}\")\n",
    "\n",
    "    # Manually updating the weights\n",
    "    # since w happen to be a tensor anything we do will affect its gradient\n",
    "    # hence this with torch.no_grad is very important here\n",
    "\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w_grad\n",
    "        b -= learning_rate * b_grad\n",
    "   \n",
    "#finally check the weight and bias\n",
    "print(f\"\\n\\nWeights: {w} \\n\\nBias: {b}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbf98a2-c99d-4196-966d-19c5611952af",
   "metadata": {},
   "source": [
    "Notice that both Weights and Bias are tensors themselves. If you want to access the actual value of a tensor and if you know that it has only 1 element then you can use the item() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a4e859ba-248a-489e-a6ef-21fb69df9693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Weight: 1.2901877164840698 \n",
      "\n",
      "Bias: 23.998760223388672\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n\\nWeight: {w.item()} \\n\\nBias: {b.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edb4f84",
   "metadata": {
    "id": "0edb4f84"
   },
   "source": [
    "In the example above, even though we were using Pytorch and tensors, we were still trying to compute gradient manually. The whole point of using tensors is it's *Autograd* feature. In our example the requires_grad property of the X_train and Y_train was set to \"False\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "156e3205-84ed-4077-9671-882c82851822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "X_train.requires_grad: False   Y_train.requires_grad: False\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n\\nX_train.requires_grad: {X_train.requires_grad}   Y_train.requires_grad: {Y_train.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5e6359-9948-422c-bc14-94c52f72d7bc",
   "metadata": {},
   "source": [
    "Now we will start using the *Autograd* feature which is the proper Pytorch way and we will also start using some of the basic Neural Network constructs from Pytorch. Instead of we maintaining two tensors for weight and bias, the linear transformation provided by torch.nn.Linear can maintain its own weights and bias internally. First we will start by using its internal weight but not the bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077cdffa",
   "metadata": {
    "id": "077cdffa"
   },
   "source": [
    "### Pytorch Code 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9b659b7e",
   "metadata": {
    "id": "9b659b7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, Loss=889.0933374490211\n",
      "Iter: 400, Loss=2.696017821172194\n",
      "Iter: 800, Loss=0.05439589078664055\n",
      "Iter: 1200, Loss=0.001097653136043394\n",
      "Iter: 1600, Loss=2.2126491664777967e-05\n",
      "Iter: 2000, Loss=4.47534672195896e-07\n",
      "\n",
      "Bias: tensor([[23.9987]], requires_grad=True) \n",
      "Weights: \n",
      "Parameter containing:\n",
      "tensor([[1.2902]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "x_train = np.float32(np.random.rand(50000,1)*10)\n",
    "\n",
    "#Synthesize training data; we will verify the weights and bias later with the trained model\n",
    "\n",
    "def synthesize_output(input):\n",
    "    return (1.29*input[0] + 24)\n",
    "\n",
    "y_train = np.array([synthesize_output(row) for row in x_train]).reshape(-1,1)\n",
    "\n",
    "X_train = torch.from_numpy(x_train)\n",
    "Y_train = torch.from_numpy(y_train)\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "# No need to Initialize the Weights as nn.Linear will maintain weights internally\n",
    "# notice bias = False, we are not using its internal bias\n",
    "model = torch.nn.Linear(1, 1, bias = False)\n",
    "\n",
    "# let us still use our own bias tensor\n",
    "b = torch.rand(1, 1, requires_grad=True)\n",
    "\n",
    "for iter in range(2001):\n",
    "\n",
    "    # forward pass : predict values\n",
    "    y_pred = model(X_train).clamp(min=0).add(b)\n",
    "\n",
    "    # find loss\n",
    "    loss = (y_pred - Y_train).pow(2).mean()\n",
    "\n",
    "\n",
    "    # Backword pass for computing gradients; Here we are using the Pytorch tensor\n",
    "    # to automatically compute the gradient for us\n",
    "    loss.backward()\n",
    "\n",
    "    #Just printing the loss to see how it is changing over the iterations\n",
    "    if (iter % 400) == 0:\n",
    "         print(f\"Iter: {iter}, Loss={loss}\")\n",
    "\n",
    "\n",
    "    #Manually updating weights\n",
    "    with torch.no_grad():\n",
    "        b -= learning_rate * b.grad\n",
    "        # Since the weights are now encapsulated inside the Linear Model m\n",
    "        # this is how you update the weights; the Pytorch way\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "            # since the weights ( params) are maintained internally by the model m\n",
    "            # we must reset them to zero after every pass\n",
    "            # it can done by directly calling m.zero_grad()\n",
    "        param.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "\n",
    "#finally check the weight and bias\n",
    "print(f\"\\nBias: {b} \\nWeights: \")\n",
    "\n",
    "for param in model.parameters():\n",
    "            print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de049137",
   "metadata": {
    "id": "de049137"
   },
   "source": [
    "In the following example now we are not maintaining the bias tensor externally, nn.Linear model will maintain the weight and bias tensor internally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ceb9fd0",
   "metadata": {
    "id": "2ceb9fd0"
   },
   "source": [
    "### Pytorch Code 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "30f6f14d",
   "metadata": {
    "id": "30f6f14d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, Loss=661.9574985984166\n",
      "Iter: 400, Loss=2.5643893082382605\n",
      "Iter: 800, Loss=0.05130146661617202\n",
      "Iter: 1200, Loss=0.0010264186666340284\n",
      "Iter: 1600, Loss=2.054430014001106e-05\n",
      "Iter: 2000, Loss=4.1591298090258304e-07\n",
      "\n",
      "Bias: tensor([23.9987]) \n",
      "Weight: tensor([[1.2902]])\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[1.2902]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([23.9987], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "x_train = np.float32(np.random.rand(50000,1)*10)\n",
    "\n",
    "#Synthesize training data; we will verify the weights and bias later with the trained model\n",
    "\n",
    "def synthesize_output(input):\n",
    "    return (1.29*input[0] + 24)\n",
    "\n",
    "y_train = np.array([synthesize_output(row) for row in x_train]).reshape(-1,1)\n",
    "\n",
    "X_train = torch.from_numpy(x_train)\n",
    "Y_train = torch.from_numpy(y_train)\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "# No need to Initialize the Weights or even the bias now as nn.Linear will maintain weights & bias internally\n",
    "m = torch.nn.Linear(1, 1, bias = True)\n",
    "\n",
    "for iter in range(2001):\n",
    "\n",
    "    # forward pass : predict values\n",
    "    y_pred = m(X_train).clamp(min=0)\n",
    "\n",
    "    # find loss\n",
    "    loss = (y_pred - Y_train).pow(2).mean()\n",
    "\n",
    "    # Backword pass for computing gradients; Here we are using the Pytorch tensor\n",
    "    # to automatically compute the gradient for us\n",
    "    loss.backward()\n",
    "\n",
    "    #Just printing the loss to see how it is changing over the iterations\n",
    "    if (iter % 400) == 0:\n",
    "         print(f\"Iter: {iter}, Loss={loss}\")\n",
    "\n",
    "    #Manually updating weights\n",
    "    with torch.no_grad():\n",
    "        # b -= learning_rate * b.grad\n",
    "        # Since the weights are now encapsulated inside the Linear Model m\n",
    "        # this is how you update the weights; the Pytorch way\n",
    "        for param in m.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "\n",
    "        m.zero_grad()\n",
    "\n",
    "# finally check the weight and bias\n",
    "print(f\"\\nBias: {m.state_dict()['bias']} \\nWeight: {m.state_dict()['weight']}\\n\")\n",
    "\n",
    "# the weights and bias ( if present ) can also be accessed as params\n",
    "for param in m.parameters():\n",
    "            print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c06c974",
   "metadata": {
    "id": "3c06c974"
   },
   "source": [
    "Now we will start using the torch.nn.Sequential container to create a more complex layered model. It takes in the input and chains various functions. In our case it feeds the input to a nn.Linear and passes the output to the ReLU() function as an input. This ReLU() is Rectified Linear Unit, which as you will see is nothing but our clip or clamp function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84213a2a",
   "metadata": {
    "id": "84213a2a"
   },
   "source": [
    "### Pytorch Code 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ea017a96",
   "metadata": {
    "id": "ea017a96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, Loss=935.9790140963855\n",
      "Iter: 400, Loss=2.5787700305199546\n",
      "Iter: 800, Loss=0.051298796767995855\n",
      "Iter: 1200, Loss=0.0010207113129141296\n",
      "Iter: 1600, Loss=2.032542230729029e-05\n",
      "Iter: 2000, Loss=4.104189362297904e-07\n",
      "\n",
      "Bias: 23.99872398376465 \n",
      "Weights:\n",
      "Parameter containing:\n",
      "tensor([[1.2902]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([23.9987], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "x_train = np.float32(np.random.rand(50000,1)*10)\n",
    "\n",
    "# Synthesize training data; we will verify the weights and bias later with the trained model\n",
    "\n",
    "def synthesize_output(input):\n",
    "    return (1.29*input[0] + 24)\n",
    "\n",
    "y_train = np.array([synthesize_output(row) for row in x_train]).reshape(-1,1)\n",
    "\n",
    "X_train = torch.from_numpy(x_train)\n",
    "Y_train = torch.from_numpy(y_train)\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "model = torch.nn.Sequential( torch.nn.Linear(1,1), torch.nn.ReLU())\n",
    "\n",
    "for iter in range(2001):\n",
    "\n",
    "    y_pred = model(X_train)\n",
    "\n",
    "    # find loss\n",
    "    loss = (y_pred - Y_train).pow(2).mean()\n",
    "\n",
    "    # as you can see you can reset the gradients to zero here or after the backward pass\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backword pass for computing gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # Just printing the loss to see how it is changing over the iterations\n",
    "    if (iter % 400) == 0:\n",
    "         print(f\"Iter: {iter}, Loss={loss}\")\n",
    "\n",
    "\n",
    "    # still updating the weights Manually\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "\n",
    "# finally check the weight and bias\n",
    "\n",
    "print(f\"\\nBias: {model[0].bias.item()} \\nWeights:\")\n",
    "\n",
    "for param in model.parameters():\n",
    "            print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2273f53d",
   "metadata": {
    "id": "2273f53d"
   },
   "source": [
    "We are almot there, now instead of manually adjusting the weights we will use a Stochastic gradient descent Optimizer or SGD Optimizer. At first it sounds a bit too intimidating but now we know what it is really doing inside, it is just tweaking the weights and bias with the specified learning rate. Also we will use the torch.nn.MSELoss loss function instead of manually calculating the loss. This will make it a full blown Pytorch example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096ecd04",
   "metadata": {
    "id": "096ecd04"
   },
   "source": [
    "### Pytorch Code 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7abe29b7",
   "metadata": {
    "id": "7abe29b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 400, Loss=2.707137107849121\n",
      "Iter: 800, Loss=0.05421265587210655\n",
      "Iter: 1200, Loss=0.0010858020978048444\n",
      "Iter: 1600, Loss=2.177524584112689e-05\n",
      "Iter: 2000, Loss=4.3822794282277755e-07\n",
      "Iter: 2400, Loss=9.072453011071957e-09\n",
      "Iter: 2800, Loss=9.072453011071957e-09\n",
      "Iter: 3200, Loss=9.072453011071957e-09\n",
      "Iter: 3600, Loss=9.072453011071957e-09\n",
      "Iter: 4000, Loss=9.072453011071957e-09\n",
      "\n",
      "Bias: 23.99980926513672 \n",
      "\n",
      "Weight: tensor([[1.2900]])\n",
      " Params:\n",
      "Parameter containing:\n",
      "tensor([[1.2900]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([23.9998], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "x_train = np.float32(np.random.rand(50000,1)*10)\n",
    "\n",
    "#Synthesize training data; we will verify the weights and bias later with the trained model\n",
    "\n",
    "def synthesize_output(input):\n",
    "    return (1.29*input[0] + 24)\n",
    "\n",
    "# Note the dtype='f' at the end it was not there in earlier example, this is a minor nuiance of using the library loss function,\n",
    "# if you omit that the loss() function will give you an error\n",
    "# alternatively you can just change he y_train data type while passing to the loss function\n",
    "# like this   loss = loss_fn(y_pred, Y_train.float())\n",
    "\n",
    "y_train = np.array([synthesize_output(row) for row in x_train], dtype='f').reshape(-1,1)\n",
    "\n",
    "X_train = torch.from_numpy(x_train)\n",
    "Y_train = torch.from_numpy(y_train)\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "model = torch.nn.Sequential( torch.nn.Linear(1,1), torch.nn.ReLU())\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "for iter in range(1, 4001):\n",
    "\n",
    "    y_pred = model(X_train)\n",
    "\n",
    "    # find loss\n",
    "    loss = loss_fn(y_pred, Y_train)\n",
    "\n",
    "    # Backword pass for computing gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # Just printing the loss to see how it is changing over the iterations\n",
    "    if (iter % 400) == 0:\n",
    "         print(f\"Iter: {iter}, Loss={loss}\")\n",
    "\n",
    "    # No more manually adjusting the weights and bias\n",
    "    optimizer.step()\n",
    "    model.zero_grad()\n",
    "\n",
    "# finally check the weight and bias\n",
    "\n",
    "print(f\"\\nBias: {model[0].bias.item()} \\n\")\n",
    "\n",
    "print(f\"Weight: {model[0].state_dict()['weight']}\\n Params:\")\n",
    "\n",
    "for param in model.parameters():\n",
    "            print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89237985",
   "metadata": {
    "id": "89237985"
   },
   "source": [
    "Now the last step. Using the Object Oriented Paradigm. Here we will declare our own class for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dcdaa7",
   "metadata": {
    "id": "a9dcdaa7"
   },
   "source": [
    "### The Full Code: OOP Way\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4aecfb5",
   "metadata": {
    "id": "b4aecfb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, Loss=933.9694213867188\n",
      "Iter: 400, Loss=2.976010322570801\n",
      "Iter: 800, Loss=0.059272490441799164\n",
      "Iter: 1200, Loss=0.0011803177185356617\n",
      "Iter: 1600, Loss=2.351148214074783e-05\n",
      "Iter: 2000, Loss=4.658043337713025e-07\n",
      "\n",
      "Bias: 23.998640060424805 \n",
      "\n",
      "Weight: tensor([[1.2902]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "x_train = np.float32(np.random.rand(50000,1)*10)\n",
    "\n",
    "# Synthesize training data; we will verify the weights and bias later with the trained model\n",
    "\n",
    "def synthesize_output(input):\n",
    "    return (1.29*input[0] + 24)\n",
    "\n",
    "y_train = np.array([synthesize_output(row) for row in x_train]).reshape(-1,1)\n",
    "\n",
    "X_train = torch.from_numpy(x_train)\n",
    "Y_train = torch.from_numpy(y_train)\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "# our own class representing the Linear Regression Model\n",
    "# it is derived from nn.Module\n",
    "class My_Linear_Regression(nn.Module):\n",
    "    # this is almost like a constructor\n",
    "    def __init__(self):\n",
    "        super().__init__() # this is like calling the base class constructor\n",
    "        self.network = torch.nn.Sequential( torch.nn.Linear(1,1), torch.nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Actual Linear Regression Code\n",
    "\n",
    "model = My_Linear_Regression()\n",
    "loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "for iter in range(2001):\n",
    "\n",
    "    y_pred = model(X_train)\n",
    "\n",
    "    # find loss\n",
    "    loss = loss_fn(y_pred, Y_train.float())\n",
    "\n",
    "    model.zero_grad()\n",
    "    #Backword pass for computing gradients\n",
    "    loss.backward()\n",
    "\n",
    "    #Just printing the loss to see how it is changing over the iterations\n",
    "    if (iter % 400) == 0:\n",
    "         print(f\"Iter: {iter}, Loss={loss}\")\n",
    "\n",
    "    # Optimize the weights and bias using the SGD Optimizer\n",
    "    optimizer.step()\n",
    "    model.zero_grad()\n",
    "\n",
    "# finally check the weight and bias\n",
    "\n",
    "print(f\"\\nBias: {model.network[0].bias.item()} \\n\")\n",
    "\n",
    "print(f\"Weight: {model.network[0].state_dict()['weight']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c37c1cb",
   "metadata": {
    "id": "1c37c1cb"
   },
   "source": [
    "### One Last Code Example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c79cb2b",
   "metadata": {
    "id": "4c79cb2b"
   },
   "source": [
    "As you can see above to keep things really simple we always used a single variable, when you are dealing with real life problems or a practical neural network example, there always will be a lot more variables than what we have seen here. The nn.Linear class, which is the basic building block, is perfectly capable of taking any number of inputs and delivering an output which we can then pass on to the next layer, it does not have to be a ReLU() it can be something much more sophisticated. The nn.Sequential can chain many such constructs to achieve something that works fantastically but nobody really understands :-) <br/> So to give you a taste of this multi variable Neural Network here is the last program that models the following equation: $ y = 4.44x_{4} + 3.33x_{3} + 2.22x_{2} + 1.11x_{1} + 12.34 $  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "85dabf49",
   "metadata": {
    "id": "85dabf49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, Loss=4921.56298828125\n",
      "Iter: 400, Loss=5.5696210861206055\n",
      "Iter: 800, Loss=3.035254955291748\n",
      "Iter: 1200, Loss=1.6541084051132202\n",
      "Iter: 1600, Loss=0.9014352560043335\n",
      "Iter: 2000, Loss=0.4912509620189667\n",
      "Iter: 2400, Loss=0.2677142322063446\n",
      "Iter: 2800, Loss=0.14589495956897736\n",
      "Iter: 3200, Loss=0.07950828969478607\n",
      "Iter: 3600, Loss=0.04332922771573067\n",
      "Iter: 4000, Loss=0.023612847551703453\n",
      "Iter: 4400, Loss=0.012868424877524376\n",
      "Iter: 4800, Loss=0.007012772373855114\n",
      "Iter: 5200, Loss=0.0038216852117329836\n",
      "Iter: 5600, Loss=0.0020827155094593763\n",
      "Iter: 6000, Loss=0.0011350527638569474\n",
      "Iter: 6400, Loss=0.0006185754318721592\n",
      "Iter: 6800, Loss=0.0003371289058122784\n",
      "Iter: 7200, Loss=0.00018373165221419185\n",
      "Iter: 7600, Loss=0.00010012908751377836\n",
      "Iter: 8000, Loss=5.4566040489589795e-05\n",
      "\n",
      "Bias: 12.313308715820312 \n",
      "\n",
      "Weight: tensor([[1.1112, 2.2212, 3.3312, 4.4412]])\n",
      "Params:\n",
      "Parameter containing:\n",
      "tensor([[1.1112, 2.2212, 3.3312, 4.4412]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([12.3133], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "x_train = np.float32(np.random.rand(50000,4)*10)\n",
    "\n",
    "# Synthesize training data; we will verify the weights and bias later with the trained model\n",
    "def synthesize_output(input):\n",
    "    return (1.11*input[0] + 2.22*input[1] + 3.33*input[2] + 4.44*input[3] + 12.34)\n",
    "\n",
    "y_train = np.array([synthesize_output(row) for row in x_train], dtype=\"f\").reshape(-1,1)\n",
    "\n",
    "X_train = torch.from_numpy(x_train)\n",
    "Y_train = torch.from_numpy(y_train)\n",
    "\n",
    "# class representing the Linear Regression Model\n",
    "\n",
    "class My_Linear_Regression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # This is our 2 Layer NN\n",
    "        self.network = torch.nn.Sequential( torch.nn.Linear(4,1), torch.nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Why do you need to change the learning_rate ??\n",
    "learning_rate = 0.005\n",
    "\n",
    "model = My_Linear_Regression()\n",
    "loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "\n",
    "for iter in range(8001):\n",
    "\n",
    "    y_pred = model(X_train)\n",
    "\n",
    "    #find loss\n",
    "    loss = loss_fn(y_pred, Y_train)\n",
    "\n",
    "    model.zero_grad()\n",
    "    #Backword pass for computing gradients\n",
    "    loss.backward()\n",
    "\n",
    "    #Just printing the loss to see how it is changing over the iterations\n",
    "    if (iter % 400) == 0:\n",
    "         print(f\"Iter: {iter}, Loss={loss}\")\n",
    "\n",
    "    #Optimize the weaights and bias\n",
    "    optimizer.step()\n",
    "    model.zero_grad()\n",
    "\n",
    "#finally check the weight and bias\n",
    "\n",
    "print(f\"\\nBias: {model.network[0].bias.item()} \\n\")\n",
    "\n",
    "print(f\"Weight: {model.network[0].state_dict()['weight']}\\nParams:\")\n",
    "\n",
    "for param in model.network.parameters():\n",
    "            print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6915fe9",
   "metadata": {
    "id": "f6915fe9"
   },
   "source": [
    "Hmmm, we did not get the Bias accurately this time, also if you have noticed above, I had to change the learning rate to 0.005 and the epoch ( number of iterations ) are changed from 4001 to 8001. Why ?? The reason is that the previous learning rate and the number of epochs simply will not work here.\n",
    "\n",
    "Now check the following code, even if we half the epochs i.e. make it 4000 instead of 8000 we still get better accuracy if we use the Adam optimizer. If you observe carefully I have also changed the learning rate to 0.5. Adam uses adaptive learning rate instead of using a constant learning rate. It uses an algorithm called momentum to speed up the gradient descent and hence finds the minima faster than the SGD Optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "572320de",
   "metadata": {
    "id": "572320de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, Loss=4655.3154296875\n",
      "Iter: 400, Loss=0.1752150058746338\n",
      "Iter: 800, Loss=9.849809430306777e-05\n",
      "Iter: 1200, Loss=1.2119466541449242e-09\n",
      "Iter: 1600, Loss=3.936969847351257e-11\n",
      "Iter: 2000, Loss=3.059649408831966e-11\n",
      "Iter: 2400, Loss=2.2863096832415053e-11\n",
      "Iter: 2800, Loss=2.245295095681943e-11\n",
      "Iter: 3200, Loss=1.0858893645382395e-11\n",
      "Iter: 3600, Loss=1.0858893645382395e-11\n",
      "Iter: 4000, Loss=4.85895733370878e-11\n",
      "Bias: 12.339998245239258 \n",
      "\n",
      "Weight: tensor([[1.1100, 2.2200, 3.3300, 4.4400]])\n",
      " Params:\n",
      "Parameter containing:\n",
      "tensor([[1.1100, 2.2200, 3.3300, 4.4400]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([12.3400], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "x_train = np.float32(np.random.rand(50000,4)*10)\n",
    "\n",
    "# Synthesize training data; we will verify the weights and bias later with the trained model\n",
    "def synthesize_output(input):\n",
    "    return (1.11*input[0] + 2.22*input[1] + 3.33*input[2] + 4.44*input[3] + 12.34)\n",
    "\n",
    "y_train = np.array([synthesize_output(row) for row in x_train], dtype=\"f\").reshape(-1,1)\n",
    "\n",
    "X_train = torch.from_numpy(x_train)\n",
    "Y_train = torch.from_numpy(y_train)\n",
    "\n",
    "# class representing the Linear Regression Model\n",
    "\n",
    "class My_Linear_Regression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # This is our 2 Layer NN\n",
    "        self.network = torch.nn.Sequential( torch.nn.Linear(4,1), torch.nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Why do you need to change the learning_rate ??\n",
    "learning_rate = 0.5\n",
    "\n",
    "model = My_Linear_Regression()\n",
    "loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "# Let us use a different Optimizer instead of SGD let us use Adam\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "for iter in range(4001):\n",
    "\n",
    "    y_pred = model(X_train)\n",
    "\n",
    "    #find loss\n",
    "    loss = loss_fn(y_pred, Y_train)\n",
    "\n",
    "    model.zero_grad()\n",
    "    #Backword pass for computing gradients\n",
    "    loss.backward()\n",
    "\n",
    "    #Just printing the loss to see how it is changing over the iterations\n",
    "    if (iter % 400) == 0:\n",
    "         print(f\"Iter: {iter}, Loss={loss}\")\n",
    "\n",
    "    #Optimize the weaights and bias\n",
    "    optimizer.step()\n",
    "    model.zero_grad()\n",
    "\n",
    "#finally check the weight and bias\n",
    "\n",
    "print(f\"Bias: {model.network[0].bias.item()} \\n\")\n",
    "\n",
    "print(f\"Weight: {model.network[0].state_dict()['weight']}\\n Params:\")\n",
    "\n",
    "for param in model.network.parameters():\n",
    "            print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6447fb8a",
   "metadata": {
    "id": "6447fb8a"
   },
   "source": [
    "## About Epochs and Minibatches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c6cf84",
   "metadata": {
    "id": "68c6cf84"
   },
   "source": [
    "Now consdier the following code here we have changed the epochs to a considerably smaller number, from 4000 down to a mere 100. But we still get the same accuracy. That is because even if the epochs have been reduced we make sure that we are adjusting the weights exactly the same number of times as before.\n",
    "\n",
    "An __\"epoch\"__ is when you have gone through the entire training dataset once and completed the forward and backward pass on the entire training data. So if you have a huge dataset then you will need to go through the entire dataset before you get a chance to update your weights.\n",
    "\n",
    "If instead of going through the the entire training dataset, we start updating the weights after a small subset of the training data then we can update the weights many more times. So we divide our training data into smaller subsets. Each subset is called a __\"minibatch\"__. We adjust the weights after every minibatch. So if you divide your entire training dataset into smaller subset, which is let us say is 10 times smaller than the training data, then you can adjust the weights 10 times during just one epoch. Hence now you can even reduce the epochs by a factor of 10. Take this with a pinch of salt, but you get the general idea. Now check the code below, now we have only 100 epoch and our minibatch size is 50.\n",
    "\n",
    "NOTE: Pytorch provides a DataLoader class which is typically used to create minibatches and iterate over the entire training dataset. But for us to use DataLoader we must first wrap our training data in a DataSet class. It is not that hard but for the sake of simplicity instead of implementing our own custom dataset we will just use another \"for\" loop to demonstrate the minibatches concept. In real life we always use a DataLoader, with an appropriate minibatch size, as it also makes it possible to deal with really humongus datasets with whatever finite RAM size we may have at our disposal. In other words it reduces the memory requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5b1282bf",
   "metadata": {
    "id": "5b1282bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 10 loss: 0.1966669261455536\n",
      "iter: 20 loss: 0.00014466181164607406\n",
      "iter: 30 loss: 2.3752364519680214e-09\n",
      "iter: 40 loss: 3.667082609792871e-11\n",
      "iter: 50 loss: 1.6007107098148232e-11\n",
      "iter: 60 loss: 1.338776225295879e-11\n",
      "iter: 70 loss: 1.222360841018899e-11\n",
      "iter: 80 loss: 1.3678800279970371e-11\n",
      "iter: 90 loss: 7.639755147947902e-12\n",
      "iter: 100 loss: 1.6880221179182975e-11\n",
      "\n",
      "\n",
      "Bias: 12.339997291564941 \n",
      "\n",
      "Weight: tensor([[1.1100, 2.2200, 3.3300, 4.4400]])\n",
      " \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "x_train = np.float32(np.random.rand(50000,4)*10)\n",
    "\n",
    "# Synthesize training data; we will verify the weights and bias later with the trained model\n",
    "def synthesize_output(input):\n",
    "    return (1.11*input[0] + 2.22*input[1] + 3.33*input[2] + 4.44*input[3] + 12.34)\n",
    "\n",
    "y_train = np.array([synthesize_output(row) for row in x_train], dtype=\"f\").reshape(-1,1)\n",
    "\n",
    "X_train = torch.from_numpy(x_train)\n",
    "Y_train = torch.from_numpy(y_train)\n",
    "\n",
    "# class representing the Linear Regression Model\n",
    "\n",
    "class My_Linear_Regression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # This is our 2 Layer NN\n",
    "        self.network = torch.nn.Sequential( torch.nn.Linear(4,1), torch.nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Why do you need to change the learning_rate ??\n",
    "learning_rate = 0.5\n",
    "\n",
    "model = My_Linear_Regression()\n",
    "loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "# Let us use a different Optimizer instead of SGD let us use Adam\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "for iter in range(1,101):\n",
    "   for mbi in range(1,2001,50):\n",
    "            y_pred = model(X_train[mbi:(mbi+50)])\n",
    "\n",
    "            #find loss\n",
    "            loss = loss_fn(y_pred, Y_train[mbi:(mbi+50)])\n",
    "\n",
    "            #Backword pass for computing gradients\n",
    "            loss.backward()\n",
    "\n",
    "            #Optimize the weaights and bias\n",
    "            optimizer.step()\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "   if(iter % 10 == 0):\n",
    "       print(f\"iter: {iter} loss: {loss}\")\n",
    "\n",
    "#finally check the weight and bias\n",
    "\n",
    "print(f\"\\n\\nBias: {model.network[0].bias.item()} \\n\")\n",
    "\n",
    "print(f\"Weight: {model.network[0].state_dict()['weight']}\\n \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8835ce6",
   "metadata": {
    "id": "a8835ce6"
   },
   "source": [
    "As you can see above we achived the same level of accuracy using a different Optimizer and changing the learning rate and changing the epochs and dividing it in smaller minitbacthes.\n",
    "\n",
    "So now the important qustion is how do you decide what should be the learning rate and how many epochs do we need? How to choose the correct optimizer and how to decide the minibact size? How do you find these optimal values to get good results ?\n",
    "\n",
    "Now that my friend is a Black Art, or in other words even I dont know all the answers yet, I am also still learning.\n",
    "<br/>One more thing to notice here is that so far the data was fabricated by us and we had some idea about the relation between the inputs and output. But what if in the real world we get some datadump that follows a complex mathematical model like:<br/></br>\n",
    "  $y = 5.3x_{1}^{3} + x_{2}^{-1} + 32x_{3}^{5}+ 3x_{3}^{-2}+24x_{3}+ (x_{4} + x_{2})^{2} + 345$ </br></br>\n",
    "What do you do in such a case? Trial and Error? Keep expanding every input variable? Remember that we will not have any idea whatsoever about the underlying mathematical model in this case. So how do you go about solving such a real life problem? Can adding multiple hidden layers help us in such situations?\n",
    "<br/>So far our Neural Network model was so basic that it was not really a network as such. The problems we were dealing with did not really require multiple layers. But we may have to use multiple hidden layers to solve the the next set of problems  <br/><br/>Let us try to understand another important class of problems termed as \"Classification\" a.k.a. \"Logistic Regression\" in the next Section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560340ec",
   "metadata": {
    "id": "560340ec"
   },
   "source": [
    "# Classification Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb25bbaa",
   "metadata": {
    "id": "bb25bbaa"
   },
   "source": [
    "Whenever people talk about classification problems or Logistic Regression problems, they think of MNIST Digit Recognition example. That is a really kool program to try but since every digit consists of multiple pixels and pixels can have different gray levels, the input data soon becomes an hindrance in understanding the core logic. So let us start with something much simpler but good enough to prove the point. First let us try the SVM and KNeighbours Classifiers from sklearn and then we will dive into Pytorch. In the end we will try to attempt the famous MNIST Digit Recognition problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eef5b4",
   "metadata": {
    "id": "59eef5b4"
   },
   "source": [
    "## The sklearn Classifier way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "963193ae",
   "metadata": {
    "id": "963193ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y data shape (20000,) y_label shape: (20000,)\n",
      "Accuracy KN:  0.958\n",
      "Accuracy SVM:  0.99\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "x_data = np.float32(np.random.rand(20000,5)*10)\n",
    "\n",
    "# This is our actual classifier function that decides one of the 4 possible states\n",
    "# based on some logic which is not that simple but follows some logic\n",
    "\n",
    "def synthesize_output(input):\n",
    "    if ( input[0] > 5):\n",
    "        return \"red\"\n",
    "\n",
    "    if ( input[1] + input[2]) > 10:\n",
    "        return \"blue\"\n",
    "\n",
    "    if ( input[1] * 2 ) < 7 :\n",
    "        return \"red\"\n",
    "    if ( input[4] > input[0]):\n",
    "        return \"black\"\n",
    "\n",
    "    if (input[2] < 3):\n",
    "        return \"green\"\n",
    "    if (input[3] - input[2]) > 1 :\n",
    "        return \"green\"\n",
    "    else:\n",
    "        return \"black\"\n",
    "\n",
    "y_data = np.array([synthesize_output(row) for row in x_data]).reshape(-1,1).flatten()\n",
    "\n",
    "# Our labels are strings like \"red\",\"blue\",\"green\",\"black\"\n",
    "# So let us use labelEncoder to convert these string values into numbers\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_data)\n",
    "y_labels = le.transform(y_data)\n",
    "print(f'Y data shape {y_data.shape} y_label shape: {y_labels.shape}')\n",
    "\n",
    "# Now we have proper data all inputs as well as the outputs are numbers\n",
    "\n",
    "# Let us Train our classifier on the first 19500 images:\n",
    "# we could have also used the sklearn.model_selection.train_test_split() function here\n",
    "# but let us keep things simple here\n",
    "\n",
    "train_x = x_data[:19500]\n",
    "# In case you dont understand what the heck is happening here in the above line\n",
    "# I highly reccommend going through https://www.pythonlikeyoumeanit.com/\n",
    "# We are just spliting the data, or more appropriately just taking the first 19500 samples\n",
    "# I know it is not the recommonded 70:30 split but just go with it for now\n",
    "\n",
    "train_y = y_labels[:19500]\n",
    "\n",
    "# clf_KN = KNeighborsClassifier()\n",
    "clf_KN = KNeighborsClassifier(n_neighbors=6,weights='distance')\n",
    "\n",
    "# clf_SVM = svm.SVC(gamma=0.001)\n",
    "clf_SVM = svm.SVC(gamma='scale',class_weight='balanced',C=100)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "clf_KN.fit(train_x, train_y)\n",
    "clf_SVM.fit(train_x, train_y)\n",
    "\n",
    "\n",
    "# predict the labels for next 1000 inputs\n",
    "test_x = x_data[19500:20000]\n",
    "expected = y_labels[19500:20000].tolist()\n",
    "\n",
    "predicted_KN = clf_KN.predict(test_x)\n",
    "predicted_SVM = clf_SVM.predict(test_x)\n",
    "\n",
    "print(\"Accuracy KN: \", accuracy_score(expected, predicted_KN))\n",
    "print(\"Accuracy SVM: \", accuracy_score(expected, predicted_SVM))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f208720",
   "metadata": {
    "id": "8f208720"
   },
   "source": [
    "As you can see using these sklearn classifiers is really easy. SVM seems to do a pretty decent job with our dataset.<br/>\n",
    "Now let us try the same thing with Pytorch and see if we can do any better than the above algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dafe74",
   "metadata": {
    "id": "c7dafe74"
   },
   "source": [
    "## The Pytorch way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d694acf4",
   "metadata": {
    "id": "d694acf4"
   },
   "source": [
    "We will try to solve exactly the same problem as above. When you are using Pytorch or any neural network for that matter for a classification problem, then one of the most important things to understand here is that since there are 4 possible classification buckets, hence the output of such an excercise is going to be a tensor or in layman terms and array of 4, indicating the probability of each classication outcome. The one with the highest probability is the answer. Each of these probabilities are represented by one linear equation. Why linear ? Because that is the basic unit of a NN. So basically what we are saying here is this:\n",
    "\n",
    "$y_{red} = x_{0}w_{red0} + x_{1}w_{red1} + x_{2}w_{red2} + x_{3}w_{red3} + x_{4}w_{red4} + b_{red}$\n",
    "<br/>\n",
    "$y_{blue} = x_{0}w_{blue0} + x_{1}w_{blue1} + x_{2}w_{blue2} + x_{3}w_{blue3} + x_{4}w_{blue4} + b_{blue}$\n",
    "<br/>\n",
    "$y_{green} = x_{0}w_{green0} + x_{1}w_{green1} + x_{2}w_{green2} + x_{3}w_{green3} + x_{4}w_{green4} + b_{green}$\n",
    "<br/>\n",
    "$y_{black} = x_{0}w_{black0} + x_{1}w_{black1} + x_{2}w_{black2} + x_{3}w_{black3} + x_{4}w_{black4} + b_{black}$\n",
    "\n",
    "Please note that $y_{red}$ is the Probabality that the outcome will be red. If we use numbers instead of this red, blue , green and black then it will be a bit more generalized. So we will have weights like $w_{0,0},w_{0,1}, w_{0,2} $..... to all the way upto $w_{3,2},w_{3,3}$ <br>\n",
    "Which essentially means we will have 4 sets of 5 weights each and similarly 4 biases one for each outcome.\n",
    "\n",
    "Following code is intentionally very raw and does not use many of the Pytorch constructs we saw above, for example nn.Linear or OOP. Hopefully it gives a better insight into what is exactly happening inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb7e2622",
   "metadata": {
    "id": "cb7e2622"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Label[0]=red After Label Encoding = 3\n",
      "Iter: 50 Loss: 0.3650060296058655\n",
      "Iter: 100 Loss: 0.36800453066825867\n",
      "Iter: 150 Loss: 0.36966535449028015\n",
      "Iter: 200 Loss: 0.3701617419719696\n",
      "Verify Results randomly \n",
      "\n",
      "Actual Label[5]: red \n",
      "After Label Encoding: 3\n",
      "Predicted Probabalites for input[5]: tensor([0.0000, 0.9459, 0.0000, 3.6188], grad_fn=<ReluBackward0>)\n",
      "Prediction for input[5]:  3\n",
      "\n",
      "Looks like we got it right!!\n",
      "\n",
      "Overall Accuracy for testing Data :  0.832\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "x_data = np.float32(np.random.rand(20000,5)*10)\n",
    "\n",
    "# This is our actual classifier function that decides one of the 4 possible states\n",
    "# based on some logic which is not that simple but follows some logic\n",
    "\n",
    "def synthesize_output(input):\n",
    "    if ( input[0] > 5):\n",
    "        return \"red\"\n",
    "\n",
    "    if ( input[1] + input[2]) > 10:\n",
    "        return \"blue\"\n",
    "\n",
    "    if ( input[1] * 2 ) < 7 :\n",
    "        return \"red\"\n",
    "    if ( input[4] > input[0]):\n",
    "        return \"black\"\n",
    "\n",
    "    if (input[2] < 3):\n",
    "        return \"green\"\n",
    "    if (input[3] - input[2]) > 1 :\n",
    "        return \"green\"\n",
    "    else:\n",
    "        return \"black\"\n",
    "\n",
    "y_data = np.array([synthesize_output(row) for row in x_data]).reshape(-1,1).flatten()\n",
    "\n",
    "# Our labels are strings like \"red\",\"blue\",\"green\",\"black\"\n",
    "# So let us use labelEncoder to convert these string values into numbers\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_data)\n",
    "y_labels = le.transform(y_data)\n",
    "\n",
    "# So even if the actual label is like \"red\" it will become something like 3 after label encoding\n",
    "# check it yourself\n",
    "print(f'Actual Label[0]={y_data[0]} After Label Encoding = {y_labels[0]}')\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Randomly initialize weights W\n",
    "W = torch.randn(5, 4)\n",
    "W.requires_grad_()\n",
    "\n",
    "# Initialize bias b as 0s and specify that we do want Pytorch\n",
    "# automatically compute th gradient everytime we do any operation on b\n",
    "b = torch.zeros(4, requires_grad=True)\n",
    "\n",
    "# We will use the SGD Optimizer here with a learning rate of 0.1\n",
    "optimizer = torch.optim.SGD([W,b], lr=0.01)\n",
    "\n",
    "# Let us split the training data and test data\n",
    "# we will use the first 19500 as training data and the last 500 as the test data\n",
    "train_x = x_data[:19500]\n",
    "train_y = y_labels[:19500]\n",
    "\n",
    "\n",
    "# convert the training data to a tensor first\n",
    "x_tr = torch.from_numpy(train_x)\n",
    "y_tr = torch.from_numpy(train_y)\n",
    "\n",
    "# how do we decide the number of epochs? Black Art ?\n",
    "for iter in range(1,201):\n",
    "    for mb in range(1,19500,100):\n",
    "        # Zero out the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        y = torch.relu(torch.matmul(x_tr[mb:(mb+100)], W) + b)\n",
    "\n",
    "        # As we know that these are really the probabilities of the input belonging to class \"red\", \"blue\" etc,\n",
    "        # hence these values must be in the range of 0 to 1.\n",
    "        # One typically uses the softmax function from the torch.nn.functional\n",
    "        # But we can use something better than that i.e. cross_entropy function which internally already uses softmax\n",
    "        cross_entropy = F.cross_entropy(y, y_tr[mb:(mb+100)].long())\n",
    "\n",
    "        # Backward pass\n",
    "        cross_entropy.backward()\n",
    "        optimizer.step()\n",
    "    if (iter % 50 == 0):\n",
    "        print(f\"Iter: {iter} Loss: {cross_entropy.item()}\")\n",
    "\n",
    "\n",
    "# How to interpret the results\n",
    "# Let us randomly examine the fifth label and the predicted label\n",
    "print(f'Verify Results randomly \\n\\nActual Label[5]: {y_data[5]} \\nAfter Label Encoding: {y_labels[5]}')\n",
    "\n",
    "# What is predicted by out model\n",
    "y_5 = torch.relu(torch.matmul(x_tr[5], W) + b)\n",
    "print(f'Predicted Probabalites for input[5]: {y_5}')\n",
    "\n",
    "# agrmax function returns the index of the highest value\n",
    "print(f'Prediction for input[5]:  {torch.argmax(y_5)}')\n",
    "\n",
    "if (y_labels[5] == torch.argmax(y_5)):\n",
    "    print(\"\\nLooks like we got it right!!\")\n",
    "else:\n",
    "    print(\"\\nOops !! We did not get it right.\")\n",
    "\n",
    "\n",
    "# Instead of checking one by obe let us Test all the remaining 500 records\n",
    "# since the model is already trained we do not need to auto compute the gradient here  hence no_grad\n",
    "\n",
    "# Our Testing Data\n",
    "X_test = x_data[19500:20000]\n",
    "x_test = torch.from_numpy(X_test)\n",
    "\n",
    "with torch.no_grad():\n",
    "        # Forward pass\n",
    "        y_test = torch.matmul(x_test, W) + b\n",
    "\n",
    "        # the one with the highest probability is the answer\n",
    "        y_predicted_labels = torch.argmax(y_test, dim=1)\n",
    "\n",
    "\n",
    "        expected_test_labels = y_labels[19500:20000]\n",
    "        print(\"\\nOverall Accuracy for testing Data : \", accuracy_score(expected_test_labels, y_predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db15d3e9",
   "metadata": {
    "id": "db15d3e9"
   },
   "source": [
    "Let us rewrite the same code using Sequential container and add something called as Standardization to the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d722fdf7",
   "metadata": {
    "id": "d722fdf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3)\n",
      "Accuracy :  0.844\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "x_data = np.float32(np.random.rand(20000,5)*10)\n",
    "\n",
    "# This is our actual classifier function that decides one of the 4 possible states\n",
    "# based on some logic which is not that simple but follows some logic\n",
    "\n",
    "def synthesize_output(input):\n",
    "    if ( input[0] > 5):\n",
    "        return \"red\"\n",
    "\n",
    "    if ( input[1] + input[2]) > 10:\n",
    "        return \"blue\"\n",
    "\n",
    "    if ( input[1] * 2 ) < 7 :\n",
    "        return \"red\"\n",
    "    if ( input[4] > input[0]):\n",
    "        return \"black\"\n",
    "\n",
    "    if (input[2] < 3):\n",
    "        return \"green\"\n",
    "    if (input[3] - input[2]) > 1 :\n",
    "        return \"green\"\n",
    "    else:\n",
    "        return \"black\"\n",
    "\n",
    "y_data = np.array([synthesize_output(row) for row in x_data]).reshape(-1,1).flatten()\n",
    "\n",
    "# Our labels are strings like \"red\",\"blue\",\"green\",\"black\"\n",
    "# So let us use labelEncoder to convert these string values into numbers\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_data)\n",
    "y_labels = le.transform(y_data)\n",
    "\n",
    "# As we know that these are really the probabilities of the input belonging to class \"red\", \"blue\" etc,\n",
    "# hence these values must be inthe range of 0 to 1.\n",
    "# One typically uses the softmax function from the torch.nn.functional\n",
    "# But we can use something better than that i.e. cross_entropy function which internally already uses softmax\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "model = nn.Sequential( torch.nn.Linear(5,4), torch.nn.ReLU())\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "\n",
    "# Let us split the training data and test data\n",
    "# we will use the first 19500 as training data and the last 500 as the test data\n",
    "train_x = x_data[:19500]\n",
    "train_y = y_labels[:19500]\n",
    "\n",
    "# Let us standardize the data, this may help us in increasing the accuracy\n",
    "sc = preprocessing.StandardScaler()\n",
    "train_X = sc.fit_transform(train_x)\n",
    "\n",
    "# convert the training data to a tensor first\n",
    "x_tr = torch.from_numpy(train_X)\n",
    "y_tr = torch.from_numpy(train_y)\n",
    "\n",
    "# how do we decide the number of epochs? Black Art ?\n",
    "for iter in range(1,201):\n",
    "    for mb in range(1,19500,100):\n",
    "\n",
    "        # Zero out the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = model(x_tr[mb:(mb+100)])\n",
    "        cross_entropy = F.cross_entropy(y_pred, y_tr[mb:(mb+100)].long())\n",
    "\n",
    "        # Backward pass\n",
    "        cross_entropy.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    #print the loss to get an idea how it is reducing\n",
    "    if (iter % 1000 == 0):\n",
    "        print(f\"Iter: {iter} Loss: {cross_entropy.item()}\")\n",
    "\n",
    "# Test the remaining 500 records\n",
    "# since the model is already trained we do not need to autocorrect the gradients now hence no_grad\n",
    "\n",
    "X_test = x_data[19500:20000]\n",
    "X_test_std = sc.transform(X_test)\n",
    "x_test = torch.from_numpy(X_test_std)\n",
    "\n",
    "with torch.no_grad():\n",
    "        # Forward pass\n",
    "        y_test = model(x_test)\n",
    "\n",
    "        # the one with the highest probability is the answer\n",
    "        y_predicted_labels = torch.argmax(y_test, dim=1)\n",
    "\n",
    "        print(y_predicted_labels[0])\n",
    "\n",
    "        expected_test_labels = y_labels[19500:20000]\n",
    "        print(\"Accuracy : \", accuracy_score(expected_test_labels, y_predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9212680",
   "metadata": {
    "id": "c9212680"
   },
   "source": [
    "The above code was just a preparation for creating a multilater NN and as you can see the code as it is, is not any better than the previous one. But now let us just add one more layer, a hidden layer and see what happens to the accuracy. Note that we have even reduced the total number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90ff45f8",
   "metadata": {
    "id": "90ff45f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.986\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch as torch\n",
    "\n",
    "x_data = np.float32(np.random.rand(20000,5)*10)\n",
    "\n",
    "# This is our actual classifier function that decides one of the 4 possible states\n",
    "# based on some logic which is not that simple but follows some logic\n",
    "\n",
    "def synthesize_output(input):\n",
    "    if ( input[0] > 5):\n",
    "        return \"red\"\n",
    "\n",
    "    if ( input[1] + input[2]) > 10:\n",
    "        return \"blue\"\n",
    "\n",
    "    if ( input[1] * 2 ) < 7 :\n",
    "        return \"red\"\n",
    "    if ( input[4] > input[0]):\n",
    "        return \"black\"\n",
    "\n",
    "    if (input[2] < 3):\n",
    "        return \"green\"\n",
    "    if (input[3] - input[2]) > 1 :\n",
    "        return \"green\"\n",
    "    else:\n",
    "        return \"black\"\n",
    "\n",
    "y_data = np.array([synthesize_output(row) for row in x_data]).reshape(-1,1).flatten()\n",
    "\n",
    "# Our labels are strings like \"red\",\"blue\",\"green\",\"black\"\n",
    "# So let us use labelEncoder to convert these string values into numbers\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_data)\n",
    "y_labels = le.transform(y_data)\n",
    "\n",
    "# As we know that these are really the probabilities of the input belonging to class \"red\", \"blue\" etc,\n",
    "# hence these values must be inthe range of 0 to 1.\n",
    "# One typically uses the softmax function from the torch.nn.functional\n",
    "# But we can use something better than that i.e. cross_entropy function which internally already uses softmax\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "learning_rate = 0.05\n",
    "\n",
    "model = nn.Sequential( torch.nn.Linear(5,10), nn.ReLU(), torch.nn.Linear(10,4))\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "\n",
    "# Let us split the training data and test data\n",
    "# we will use the first 19500 as training data and the last 500 as the test data\n",
    "train_x = x_data[:19500]\n",
    "train_y = y_labels[:19500]\n",
    "\n",
    "sc = preprocessing.StandardScaler()\n",
    "train_X = sc.fit_transform(train_x)\n",
    "\n",
    "# convert the training data to a tensor first\n",
    "x_tr = torch.from_numpy(train_X)\n",
    "y_tr = torch.from_numpy(train_y)\n",
    "\n",
    "# how do we decide the number of epochs? Black Art ?\n",
    "for iter in range(1,100):\n",
    "    for mbiter in range(1,19500,100):\n",
    "        # Zero out the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = model(x_tr[mbiter:(mbiter+100)])\n",
    "        cross_entropy = loss_fn(y_pred, y_tr[mbiter:(mbiter+100)].long())\n",
    "\n",
    "        # Backward pass\n",
    "        cross_entropy.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    #print the loss to get an idea how it is reducing\n",
    "    ##if (iter % 50 == 0):\n",
    "     #   print(f\"Iter: {iter} Loss: {cross_entropy.item()}\")\n",
    "\n",
    "# Test the remaining 500 records\n",
    "# since the model is already trained we do not need to autocorrect the gradients now hence no_grad\n",
    "\n",
    "X_test = x_data[19500:20000]\n",
    "x_test = torch.from_numpy(X_test)\n",
    "X_test = sc.transform(x_test)\n",
    "\n",
    "with torch.no_grad():\n",
    "        # Forward pass\n",
    "        y_test = model(torch.from_numpy(X_test).float())\n",
    "\n",
    "        # the one with the highest probability is the answer\n",
    "        y_predicted_labels = torch.argmax(y_test, dim=1)\n",
    "\n",
    "        expected_test_labels = y_labels[19500:20000]\n",
    "        print(\"Accuracy : \", accuracy_score(expected_test_labels, y_predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70366edf",
   "metadata": {
    "id": "70366edf"
   },
   "source": [
    "Now with all this preparation we are finally ready to take on the MNIST Digits Recognition problem.\n",
    "## The MNIST Handwritten Digits Recognition Probelm\n",
    "<br/><br/>PytorchVision package provides many datasets, MNIST is one of these datasets. This dataset has lots of ( around 70000) images of handwritten digits along with its correct label. We are supposed to train our model to corretcly identify the digit from any given image. Each record is a tuple first part is the actual image object and the other is the label. First part is actually an object of PIL.Image class and the second part is the integer number from the range 0 to 9. Every handwritten image is 28 x 28 pixels in size. Even though the Input data is really in 2D format we flatten it to an array of 28 x 28 = 784 elements. Each pixel value represets a particular shade of gray, these values can be in the range 0 to 255.\n",
    "So basically we have 784 input variables and the output will be a tensor with 10 values where each value represents the probability the given image represents a Digit from 0 to 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31167a5d",
   "metadata": {
    "id": "31167a5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label of the 18th Image is: 6\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True)\n",
    "# Let us examine the 18th record from this training dataset\n",
    "image, label = train_dataset[18]\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "print(f'Label of the 18th Image is: {label}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aae602f-c1b9-47e8-a77b-6e9920054942",
   "metadata": {
    "id": "2aae602f-c1b9-47e8-a77b-6e9920054942"
   },
   "source": [
    "Now that we understand the MNIST dataset, Let us try to study following Pytorch code that uses single layer NN wrapped inside our own model class called \"SingleLayerModel\". Even with just one layer it gives pretty decent accuracy. It uses two new things that we have not seen before namely the DataLoader and transforms. So before we jump into the code let us quickly talk about DataLoader and Transforms.\n",
    "### About DataLoader and Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0bdcbd-ece1-4f51-a5d2-cea9b02fc722",
   "metadata": {
    "id": "1c0bdcbd-ece1-4f51-a5d2-cea9b02fc722"
   },
   "source": [
    " In most of the real life problmes the data can be massive and it may not be always possible to load the entire data in memory also as we have already seen we may want to split the training data into multiple minibatches. That is exactly what this **DataLoader** helps us to do. To use DataLoader your data must be wrapped inside a class that implements certain protocols like __iter__(), __getitem__(), __len()__ or in other words it must be derived from torch.utils.data.IterableDataset. <br/>  The training data may not be always in the most desirable format, so we may have to pre-process, massage or manipulate the data before it can be fed to the training model, in other words data must be transformed into a more suitable format. That is where the **trasnforms** comes into picture. You can also chain multiple transforms using the Compose class, to perform more than one preprocessing steps before feeding the data to your model. Several transforms like ToTensor, Normalize, Resize, CenterCrop, Grayscale, GaussianBlur are readily available in pytorch.transforms. Although you can easily do any custom transformation in your Models forward pass function itself, but if you ever need to write your own custom transform fucntion then even that is possible with the help of the transforms.Lambda() class. <br/>Both these DataLoader and Transforms are very useful and make the code smaller and manageable.<br/> Coming back to our MNIST code, we can certainly write the same code without using DataLoader and transforms, but we will first study the code with these constructs. Once you look at the other code later you will understand why we did not start with this sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8baee17a",
   "metadata": {
    "id": "8baee17a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.4702\n",
      "Epoch [2/10], Loss: 0.3907\n",
      "Epoch [3/10], Loss: 0.2660\n",
      "Epoch [4/10], Loss: 0.2588\n",
      "Epoch [5/10], Loss: 0.3872\n",
      "Epoch [6/10], Loss: 0.2883\n",
      "Epoch [7/10], Loss: 0.1398\n",
      "Epoch [8/10], Loss: 0.3325\n",
      "Epoch [9/10], Loss: 0.1927\n",
      "Epoch [10/10], Loss: 0.3945\n",
      "\n",
      "Predicted Digit is: 1 and the actual Label from the Training Data is: 1\n",
      "Accuracy: 91.76%\n"
     ]
    }
   ],
   "source": [
    "# %%timeit\n",
    "# Uncomment the above line if you want to measure the time taken by the following code\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a simple single-layer model\n",
    "class SingleLayerModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SingleLayerModel, self).__init__()\n",
    "        self.linear = nn.Linear(28 * 28, 10)  # Input size: 28x28, Output size: 10 (digits)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)  # Flatten the input\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "# Load MNIST dataset\n",
    "mytransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=mytransform, download=True)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Create the model\n",
    "model = SingleLayerModel()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "# Training is completed, let us check one random input sampel and the predicted label\n",
    "# let us check the 23th record from training data\n",
    "idata23, label23 = train_dataset[23]\n",
    "prediction = model(idata23)\n",
    "\n",
    "# prediction is a tensor with 10 values that indicate the probability of the inpute representing each digit\n",
    "# let us find out which digit has the highest probabilty using the torch.argmax, it gives you the index of the highest value\n",
    "\n",
    "print(f'\\nPredicted Digit is: {torch.argmax(prediction)} and the actual Label from the Training Data is: {label23}')\n",
    "\n",
    "# Instead of checking individual random values lets us check all the Test data and find the overall accuracy\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=mytransform, download=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = (correct / total) * 100\n",
    "print(f'Accuracy: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c61838",
   "metadata": {
    "id": "64c61838"
   },
   "source": [
    "As we can see above this works and the accuracy is also pretty good for a single layer model. In this code we are using transforms to convert the data into a Pytorch tensor and also to normalize the data. Normalizing or scaling the data to a fixed range is said to help to \"learn\" faster,  not you to learn faster but for the NN to learn faster.<br> Following code is trying to explain what the DataLoader and the trasnforms are trying to do internally. It is a lot messier than the previous code and probably will help you appreciate why the previous is much better. <br/> But you can clearly see that the below is code significantly faster because there we are avoiding the overheads added by DataLoader and also using the numpy vectorization to convert the lists into numpy array before converting them to Pytorch Tensors. You can verify it yourself by adding \"%%timeit\"   to the first line of both these examples and check the average time taken by both these programs, but just be aware that by default the **%%timeit magic** runs your code multiple times and computes the average time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c7a2ccb",
   "metadata": {
    "id": "3c7a2ccb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 39.9566\n",
      "Epoch [2/10], Loss: 59.5553\n",
      "Epoch [3/10], Loss: 7.7273\n",
      "Epoch [4/10], Loss: 28.7690\n",
      "Epoch [5/10], Loss: 30.6617\n",
      "Epoch [6/10], Loss: 25.2170\n",
      "Epoch [7/10], Loss: 21.6608\n",
      "Epoch [8/10], Loss: 9.4355\n",
      "Epoch [9/10], Loss: 39.1481\n",
      "Epoch [10/10], Loss: 0.0000\n",
      "Accuracy: 82.68%\n"
     ]
    }
   ],
   "source": [
    "# %%timeit\n",
    "# Uncomment the above line if you want to measure the time taken by the following code\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "import numpy as np\n",
    "\n",
    "# Define a simple single-layer model\n",
    "class SingleLayerModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SingleLayerModel, self).__init__()\n",
    "        self.linear = nn.Linear(28 * 28, 10)  # Input size: 28x28, Output size: 10 (digits)\n",
    "        #self.linear = torch.nn.Sequential( torch.nn.Linear(28 * 28, 64) , torch.nn.Linear(64, 10), torch.nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)  # Flatten the input\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "# Notice that we are not using the Transform here so we will have to normalize the data if we want to\n",
    "# we must convert the data to a Pytorch tensor ourselves\n",
    "mnist_train = datasets.MNIST(root=\"./data\", train=True, download=True)\n",
    "mnist_test = datasets.MNIST(root=\"./data\", train=False, download=True)\n",
    "\n",
    "# MNIST training dataset consists of 60000 tuples, first part is the image data and second part is the label\n",
    "# Let us extract all the Image inputs by separating the labels\n",
    "\n",
    "imageData_train = [data[0] for data in mnist_train]\n",
    "labels_train = [data[1] for data in mnist_train]\n",
    "\n",
    "# convert all the Image objects to np array so that we can convert it to a Tensor\n",
    "# Otherwise you can not directly convert a PIL.Image type object into a tensor\n",
    "\n",
    "imgData_train = np.array([ np.array(x) for x in imageData_train])\n",
    "\n",
    "# Convert the list into a Tensor of Tensors\n",
    "imData_train = torch.Tensor(imgData_train)[:, np.newaxis,:]\n",
    "lbls_train = torch.Tensor(labels_train).long()\n",
    "\n",
    "# instantiate the model\n",
    "model = SingleLayerModel()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for mbi in range(0,60000, batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imData_train[mbi: (mbi+batch_size)])\n",
    "        loss = criterion(outputs, lbls_train[mbi: (mbi+batch_size)])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Calculate accuracy\n",
    "\n",
    "# Extract the inputs and labels from the Test data this time\n",
    "\n",
    "imageData_test = np.array([data[0] for data in mnist_test])\n",
    "labels_test = [data[1] for data in mnist_test]\n",
    "\n",
    "# converyt all the Image objects to np array so that we can convert it to a Tensor\n",
    "# Otherwise you can not directly convert a PIL.Image type object into a tensor\n",
    "\n",
    "imgData_test = np.array([ np.array(x) for x in imageData_test])\n",
    "\n",
    "# Convert the list into a Tensor of Tensors\n",
    "imData_test = torch.Tensor(imgData_test)\n",
    "lbls_test = torch.Tensor(labels_test)\n",
    "\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "        outputs = model(imData_test)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += lbls_test.size(0)\n",
    "        correct += (predicted == lbls_test).sum().item()\n",
    "\n",
    "accuracy = (correct / total) * 100\n",
    "print(f'Accuracy: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53420611",
   "metadata": {
    "id": "53420611"
   },
   "source": [
    "As you can see the code is not as clean as the previous sample. On top of it the accuracy is also slightly less. Is that because we did not normalize the data? Let us add that last bit and see how it affects the accuracy and also how it affects the running time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e1e7efb5",
   "metadata": {
    "id": "e1e7efb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.1665\n",
      "Epoch [2/10], Loss: 0.1172\n",
      "Epoch [3/10], Loss: 0.0960\n",
      "Epoch [4/10], Loss: 0.0844\n",
      "Epoch [5/10], Loss: 0.0770\n",
      "Epoch [6/10], Loss: 0.0720\n",
      "Epoch [7/10], Loss: 0.0684\n",
      "Epoch [8/10], Loss: 0.0664\n",
      "Epoch [9/10], Loss: 0.0645\n",
      "Epoch [10/10], Loss: 0.0635\n",
      "Accuracy: 92.15%\n"
     ]
    }
   ],
   "source": [
    "# %%timeit\n",
    "# Uncomment the above line if you want to measure the time taken by the following code\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "\n",
    "# Define a simple single-layer model\n",
    "class SingleLayerModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SingleLayerModel, self).__init__()\n",
    "        self.linear = torch.nn.Sequential( torch.nn.Linear(28 * 28, 64), nn.Linear(64, 10), torch.nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)  # Flatten the input\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "# Torchvision package provides many DataSets, let us Load the MNIST dataset\n",
    "mnist_train = datasets.MNIST(root=\"./data\", train=True, download=True)\n",
    "mnist_test = datasets.MNIST(root=\"./data\", train=False, download=True)\n",
    "\n",
    "# MNIST training dataset consists of 60000 tuples, first part is the image data and second part is the label\n",
    "# Let us extract all the Image inputs by separating the labels\n",
    "\n",
    "imageData_train = [data[0] for data in mnist_train]\n",
    "labels_train = [data[1] for data in mnist_train]\n",
    "\n",
    "# convert all the Image objects to np array so that we can convert it to a Tensor\n",
    "# Otherwise you can not directly convert a PIL.Image type object into a tensor\n",
    "\n",
    "# Now instead of transforms.normalize we are using StandardScalar from sklearn\n",
    "# Please note that this is not the same as normalize((0.5),(0.5)) but this is much easier to use the standard scalar than\n",
    "# trying to mimic the above pytorch transform\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "imgdata_tr = [ np.array(x) for x in imageData_train]\n",
    "\n",
    "normalized_data_tr = [scaler.fit_transform(x) for x in imgdata_tr]\n",
    "\n",
    "imgData_train = np.array(normalized_data_tr)\n",
    "\n",
    "# Convert the list into a Tensor of Tensors\n",
    "imData_train = torch.Tensor(imgData_train)[:, np.newaxis,:]\n",
    "lbls_train = torch.Tensor(labels_train).long()\n",
    "\n",
    "# Create the model\n",
    "model = SingleLayerModel()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for mbi in range(0,60000, batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imData_train[mbi: (mbi+batch_size)])\n",
    "        loss = criterion(outputs, lbls_train[mbi: (mbi+batch_size)])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Calculate the accuracy\n",
    "\n",
    "# MNIST Test dataset consists of 10000 tuples first part is the image data and the second part is the label\n",
    "# Let use a different syntax this time, achieves the same results\n",
    "\n",
    "imageData_test = [data[0] for data in mnist_test]\n",
    "labels_test = [data[1] for data in mnist_test]\n",
    "\n",
    "\n",
    "imgdata_test = [ np.array(x) for x in imageData_test]\n",
    "\n",
    "# Notice that this additinal step of again scaling the Test data is not required when you are using the\n",
    "# transforms as the entire dataset gets normalized to begin with when we load it\n",
    "\n",
    "normalized_data_test = [scaler.fit_transform(x) for x in imgdata_test]\n",
    "\n",
    "imgData_test = np.array(normalized_data_test)\n",
    "\n",
    "# Convert the list into a Tensor of Tensors\n",
    "imData_test = torch.Tensor(imgData_test)\n",
    "lbls_test = torch.Tensor(labels_test)\n",
    "\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "        outputs = model(imData_test)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += lbls_test.size(0)\n",
    "        correct += (predicted == lbls_test).sum().item()\n",
    "\n",
    "accuracy = (correct / total) * 100\n",
    "print(f'Accuracy: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5b06aa",
   "metadata": {
    "id": "ea5b06aa"
   },
   "source": [
    "As you can see that it has increased the running time but has also increased the Accuracy. But it still faster than the transformers version above.<br/>To continue our method of trying  the same thing in sklearn as well as Pytorch NN, we should also try to rewrite the above MNIST dataset problem using the sklearn to continue our tradition. So for the sake of completeness here is the sklearn way of solving MNIST Problem using Random Forest classifier.\n",
    "## MNIST: The sklearn Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c98dd22",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8c98dd22",
    "outputId": "5a966f1b-f30e-4995-eee8-041499094ee6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.53%\n"
     ]
    }
   ],
   "source": [
    "# %%timeit\n",
    "# Uncomment the above line if you want to measure the time taken by the following code\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the MNIST dataset from scikit-learn's datasets module\n",
    "mnist = fetch_openml(name='mnist_784', parser=\"pandas\")\n",
    "\n",
    "# Extract features (X) and labels (y) from the dataset\n",
    "X = mnist.data\n",
    "y = mnist.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features (optional but often recommended)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create and train the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=60, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {100 * accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01585c15",
   "metadata": {
    "id": "01585c15"
   },
   "source": [
    "As you can see the sklearn Random Forest has a much better accuracy score. So may be we can improve our Pytorch code accuracy by adding a couple of hidden layers. <br/>Now that you have enough understanding of using Pytorch and also basic understanding of the above problem you should be able to experiment on your own to improve our MNIST Pytorch Code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f31f46-4b61-4b4b-b375-b295d8f5cd9a",
   "metadata": {
    "id": "c2f31f46-4b61-4b4b-b375-b295d8f5cd9a"
   },
   "source": [
    "## So What? / What Next ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33d479b-aee5-41b8-992e-75e24fa1af67",
   "metadata": {
    "id": "b33d479b-aee5-41b8-992e-75e24fa1af67"
   },
   "source": [
    "One school of thought goes like this: _\"Even if you do not really understand how the Internal Combustion Engine inside your vehicle works, you can still use it, you can make it driver faster or slower, you just know how to use it.\"_ <br/>So what is the point of all this, why do you need to understand all these internal details? If you just know what all things are available out there and you know how to use them in practice then that is all you need. As long as you can join the various technological pieces together and are able to solve some real world problem that is really worth solving then that is all that matters. The Open Source community and various services out there have really made the Machine Learning so much accessible. Now with the advent of Hugging Face and thousands of AI/ML services and resources, you can build really great things with just few lines of code in just few minutes. <br/> To give you a taste of the transformers; here is a small code sinpette; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "504f5aa8-feec-4c0d-92ba-e66090e38cb4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "504f5aa8-feec-4c0d-92ba-e66090e38cb4",
    "outputId": "7926da06-8327-4998-bd01-b0bfd9a86ca3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q1. What are some other names for the slingshot effect? \n",
      "Answer: planetary swing-by or a gravity-assist manoeuvre.\n",
      "\n",
      "Q2. What is the planetary swing-by used for? \n",
      "Answer: change of direction of a spacecraft as it passes close to a planet.\n",
      "\n",
      "Q3. Does gravity-assist manoeuvre help in increasing the speed or only changes the direction? \n",
      "Answer: It is performed to achieve an increase in speed\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import pytesseract\n",
    "\n",
    "\n",
    "# If you want to try running this locally on your machine\n",
    "# make sure that you install the tesseract-OCR for windows and update the actual exe path below\n",
    "pytesseract.pytesseract.tesseract_cmd = \"C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe\"\n",
    "\n",
    "# Initialize the document-question-answering pipeline\n",
    "nlp = pipeline(\"document-question-answering\", model=\"impira/layoutlm-document-qa\")\n",
    "\n",
    "# I used a local file for my experiments, if you want to try this yourself the link is given below in the description\n",
    "image_path = \"docImage.png\"\n",
    "\n",
    "question1 = \"What are some other names for the slingshot effect?\"\n",
    "result = nlp(image_path, question1)\n",
    "print(f\"\\nQ1. {question1} \\nAnswer: {result[0][\"answer\"]}\")\n",
    "\n",
    "question2 = \"What is the planetary swing-by used for?\"\n",
    "result = nlp(image_path, question2)\n",
    "print(f\"\\nQ2. {question2} \\nAnswer: {result[0][\"answer\"]}\")\n",
    "\n",
    "question3 = \"Does gravity-assist manoeuvre help in increasing the speed or only changes the direction?\"\n",
    "result = nlp(image_path, question3)\n",
    "print(f\"\\nQ3. {question3} \\nAnswer: {result[0][\"answer\"]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98403a28-da9b-4624-b1ef-63fb2c31737f",
   "metadata": {
    "id": "98403a28-da9b-4624-b1ef-63fb2c31737f"
   },
   "source": [
    "Isn't this really easy? In the above code you are reading an image file, converting it to text and then answering some question based on the contents of that converted text in just a few lines. If you want to check the image file then <a href=\"http://dl.dropboxusercontent.com/scl/fi/yh3leb1bcly4uf48nmfwe/DocImage.png?rlkey=0s0hiauf5hkhpunlxnf4hwa12&dl=0\"> here is the actual Image file .</a><br/>In just few lines we can do so many cool things like Qustion Answering, Sentiment analysis, Text Generation, Translation. There are so many paid services, free open source resources out there that really \"Imagination is the only limit\". Looking at all this you kind of tend to agree with the \"Just Make Use Of It\" school of thought. But for some pople like  me, the process of learning and understanding the workings of these things has it's own reward. Sometime when you want to improve certain things or if things are not working \"as-it-is\", then a deeper understanding might be needed. <br/> This was a brief introudction to Machine Learning using Pytroch with the main focus on Code. There is still a long way to. I hope that someone somewhere finds it useful. <br/> **Happy Learning and Happy Coding !!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7612a9-a49a-4488-995f-7b667f1dbbd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
